{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Spark DataFrames Example\n",
    "\n",
    "Web log example implemented in pure Python.\n",
    "\n",
    "This notebook exemplifies the execution of a Spark program in Python, using the DataFrames interface.\n",
    "In this example, spark runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n",
    "\n",
    "Spark documentation available at:\n",
    "https://spark.apache.org/docs/2.3.1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset \n",
    "\n",
    "Log format: <br>\n",
    "date IP_source return_value operation URL time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -O web.log https://www.dropbox.com/s/0r8902uj9yum7dg/web.log?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of source IP addresses\n",
    "\n",
    "Read the log from input and compute the intended result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('web.log')\n",
    "    l2 = lines.filter( lambda line : len(line) > 0 )\n",
    "    l3 = l2.map( lambda line : line.split(' ') )\n",
    "    logRows = l3.map( lambda arr : Row( date = arr[0], ipSource = arr[1], retValue = arr[2], \\\n",
    "                                               op = arr[3], url = arr[4], time = arr[5]))\n",
    "\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    listIpsDF = logRowsDF.select( 'ipSource').distinct()\n",
    "    \n",
    "    listIpsDF.show(100)\n",
    "\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find log entries with a given URL\n",
    "\n",
    "Read the log from input and compute the intended result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "URL = '/codemove/TTCENCUFMH3C'\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('web.log')\n",
    "    logRows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(' ') ) \\\n",
    "                        .map( lambda arr : Row( date = arr[0], ipSource = arr[1], retValue = arr[2], \\\n",
    "                                               op = arr[3], url = arr[4], time = arr[5]))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    ################ TO COMPLETE\n",
    "    \n",
    "    \n",
    "    sc.stop()\n",
    "except:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inverted index url->ips\n",
    "\n",
    "Read the log from input and compute the intended result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('web.log')\n",
    "    logRows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(' ') ) \\\n",
    "                        .map( lambda arr : Row( date = arr[0], ipSource = arr[1], retValue = arr[2], \\\n",
    "                                               op = arr[3], url = arr[4], time = arr[5]))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    ################ TO COMPLETE\n",
    "    \n",
    "    invertedIndexDF.show(100)\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inverted index ip->urls\n",
    "\n",
    "Read the log from input and compute the intended result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('words').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('web.log')\n",
    "    logRows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(' ') ) \\\n",
    "                        .map( lambda arr : Row( date = arr[0], ipSource = arr[1], retValue = arr[2], \\\n",
    "                                               op = arr[3], url = arr[4], time = arr[5]))\n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    ################ TO COMPLETE\n",
    "    \n",
    "    \n",
    "    invertedIndexDF.show(30)\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
